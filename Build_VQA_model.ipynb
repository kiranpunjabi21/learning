{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRduzoHbTjVA",
        "outputId": "ca371815-d315-40af-a4b5-df347e1d9f1b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUNmURCPTE2N",
        "outputId": "58240b70-b8ef-4e99-fcb9-d3aca508a516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to dandelin/vilt-b32-finetuned-vqa and revision 4355f59 (https://huggingface.co/dandelin/vilt-b32-finetuned-vqa).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.4866306781768799, 'answer': '3'}]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from transformers import pipeline\n",
        "\n",
        "vqa_pipeline = pipeline(\"visual-question-answering\")\n",
        "\n",
        "image =  Image.open(\"/content/images.jpeg\")\n",
        "question = \"what man is doing in image?\"\n",
        "ques2 = 'Is there any animal in image?'\n",
        "ques3 = 'How many plants are there in image?'\n",
        "vqa_pipeline(image, question, top_k=1)\n",
        "vqa_pipeline(image, ques2, top_k=1)\n",
        "vqa_pipeline(image, ques3, top_k=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, TFBertModel"
      ],
      "metadata": {
        "id": "1uIgMR65UGKV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model = TFBertModel.from_pretrained('bert-base-cased')\n",
        "ques = \"What is man doing in image?\"\n",
        "\n",
        "def ques_embed(ques):\n",
        "\n",
        "  encoded_ques = tokenizer(ques,return_tensors='tf')\n",
        "  output = model(encoded_ques)\n",
        "  #print(output)\n",
        "  return output"
      ],
      "metadata": {
        "id": "iQV8_o-VUlHT",
        "outputId": "f1ac8cd6-e2c3-4008-9c6d-e63720770bf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input\n",
        "import numpy as np\n",
        "\n",
        "model = ResNet50(include_top=False,pooling='avg')\n",
        "\n",
        "def image_embed(image_path):\n",
        "  img = image.load_img(image_path,target_size=(224,224))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x,axis=0)\n",
        "  x = preprocess_input(x)\n",
        "  pred = model.predict(x)\n",
        "  print(pred)\n",
        "  return pred"
      ],
      "metadata": {
        "id": "GXA1d64yWrvE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_embed('/content/images.jpeg')"
      ],
      "metadata": {
        "id": "REQVlI58WxIB",
        "outputId": "809756e6-94cd-4c00-ec6c-329c778df947",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "[[0.3645531  0.55496246 0.7950819  ... 1.0758163  0.8979599  0.30728483]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.3645531 , 0.55496246, 0.7950819 , ..., 1.0758163 , 0.8979599 ,\n",
              "        0.30728483]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "ques_embed_dim = 768\n",
        "image_embed_dim = 2048\n",
        "common_embed_dim = 512\n",
        "\n",
        "ques_projection = nn.Linear(ques_embed_dim,common_embed_dim)\n",
        "image_projection = nn.Linear(image_embed_dim,common_embed_dim)\n",
        "\n",
        "\n",
        "a = ques_embed(ques)\n",
        "\n",
        "# Project the embeddings\n",
        "question_embedding_projected = ques_projection(a)\n",
        "image_embedding_projected = image_projection(image_embed('/content/images.jpeg'))\n",
        "\n",
        "combined_embedding = question_embedding_projected * image_embedding_projected\n"
      ],
      "metadata": {
        "id": "4Gdvt_CxZ3NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def VQA(common_embed_dim,num_classes):\n",
        "  fc = nn.Sequential(nn.Linear(common_embed_dim,256),nn.ReLU(),nn.Linear(256,num_classes))\n",
        "  def forward(combined_embedding):\n",
        "    output = fc(combined_embedding)\n",
        "    return F.log_softmax(output,dim=1)\n",
        "\n",
        "vqa = VQA(common_embed_dim,1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LSAH8dN5gA0C"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define loss function and optimizer\n",
        "criteria = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vqa.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "vOy2VNLMzeYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = vqa_model(combined_embedding)\n"
      ],
      "metadata": {
        "id": "EVvUPGLi0j-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate loss and backpropagate\n",
        "target = torch.randint(0, 1, (1,))  # Replace with your actual ground truth label\n",
        "loss = criterion(output, target)\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "6ttGYTsv2Qj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.step()"
      ],
      "metadata": {
        "id": "JEjvN4GC2S4y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}